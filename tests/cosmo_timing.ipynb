{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_theory = {\"camb\": {\"extra_args\": {\"lens_potential_accuracy\": 1}}}\n",
    "#info_theory = {\"classy\": {\"extra_args\": {\"non linear\": \"halofit\"}}}\n",
    "\n",
    "modules = \"\"  # SET!!!\n",
    "\n",
    "n = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from collections import OrderedDict as odict\n",
    "\n",
    "from cobaya.conventions import _likelihood, _theory, _timing, _path_install\n",
    "from cobaya.tools import recursive_update\n",
    "from cobaya.cosmo_input import create_input, planck_base_model\n",
    "from cobaya.model import get_model\n",
    "\n",
    "from test_cosmo_planck_2015 import params_lowTEB_highTTTEEE\n",
    "\n",
    "info = create_input(planck_names=True, theory=list(info_theory)[0], **planck_base_model)\n",
    "info = recursive_update(info, {_theory: info_theory})\n",
    "\n",
    "likelihoods = [\n",
    "    \"planck_2015_lowl\", \"planck_2015_lowTEB\",\n",
    "    \"planck_2015_plikHM_TT\", \"planck_2015_plikHM_TTTEEE\",\n",
    "    #\"planck_2015_plikHM_TT_unbinned\", \"planck_2015_plikHM_TTTEEE_unbinned\",\n",
    "    \"planck_2015_lensing\", \"planck_2015_lensing_cmblikes\",\n",
    "    \"bicep_keck_2015\",\n",
    "    \"H0_riess2018a\", \"H0_riess2018b\",\n",
    "    \"sdss_dr12_consensus_bao\", \"sdss_dr12_consensus_full_shape\", \"sdss_dr12_consensus_final\",\n",
    "    \"sdss_dr7_mgs\", \"sixdf_2011_bao\",\n",
    "    \"sn_jla\", \"sn_jla_lite\", \"sn_pantheon\",\n",
    "    \"des_y1_shear\", \"des_y1_clustering\", \"des_y1_galaxy_galaxy\"\n",
    "]\n",
    "\n",
    "# NB: It's possible that DES makes everything else slower, since it adds many more\n",
    "# redshifts to the computation, so there are many more redshifts to select the\n",
    "# required e.g. H(z) from, making e.g. SN and BAO likes slower (longer theory code request)\n",
    "\n",
    "info[_likelihood] = odict([[l,None] for l in likelihoods])\n",
    "\n",
    "# No fsigma8 in classy!\n",
    "if list(info_theory)[0] == \"classy\":\n",
    "    info[_likelihood].pop(\"sdss_dr12_consensus_final\")\n",
    "    info[_likelihood].pop(\"sdss_dr12_consensus_full_shape\")\n",
    "\n",
    "info[_timing] = True\n",
    "info[_path_install] = modules\n",
    "model = get_model(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lowTEB_highTTTEEE.pop(\"H0\", None)\n",
    "\n",
    "for i in range(n):\n",
    "    point = dict(zip(model.parameterization.sampled_params(), model.prior.sample(ignore_external=True)[0]))\n",
    "    point.update(params_lowTEB_highTTTEEE)\n",
    "    model.loglike(point, cached=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "df = DataFrame(columns=[\"time\", \"speed\", \"old speed\"], index=likelihoods)\n",
    "for name in [_theory] + list(model.likelihood):\n",
    "    like = model.likelihood[name]\n",
    "    df.at[name, \"time\"] = like.time_avg\n",
    "    df.at[name, \"speed\"] = 1/like.time_avg\n",
    "    df.at[name, \"old speed\"] = like.speed\n",
    "    df.at[name, \"factor\"] = like.speed*like.time_avg\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
